\documentclass[10pt]{beamer}%
\usetheme{Boadilla}
\usecolortheme{seahorse}

\usepackage[utf8]{inputenc}%

% graphics
%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Doc info %%%%%%%%%%%%%%%%%%%
\title[\textbf{Linear models 2:}]{What on earth is going on with my linear models??!}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

<<Plot Options, echo=FALSE, message=FALSE>>=
#load(file = ".RData")
opts_knit$set(width=60)
opts_chunk$set(comment=NA, fig.width=8, fig.height=6, out.width='0.8\\textwidth',
               out.height='0.6\\textwidth',background='#D7DDEB', size="small")


szgr <- 2
szax <- 1.3
marr <- c(4, 4, 1, 1) + 0.1
setPar<-function(){
par(las=1,mar=marr, cex=szgr, cex.lab=szax , cex.axis=szax, lwd=2 ,pch=1, las=1)
}
setPar()
@


\begin{frame}
\maketitle	
\end{frame}
%%%%%%%%%%%

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \tableofcontents[currentsection,hideothersubsections,subsectionstyle=hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide] 
  \end{frame}
} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear model, reminder}

\begin{frame}[fragile]{A simple linear model}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\
  
  <<lmprinc, echo=FALSE, dev='tikz'>>=
    setPar()
    set.seed(123)
    x <- rnorm(20)
    y <- 1 + x + rnorm(20)
    plot(x, y, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}")
    lm0 <- lm(y~x)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    
    arrows(x0 = x, y0=y, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @ 
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\
  \vspace{1cm}
\textbf{In R:}
<<eval=FALSE>>=
  lm(response ~ 1 + predictor1 + predictor2, data=data) 
@
\begin{itemize}
  \item Intercept can be explicit or implicit
  \item Can remove intercept with \texttt{\dots $\sim $ 0 + \dots}
  \item Error is implicit
  \item Feed the option \texttt{data=} to keep code short, reliable and flexible
  \item Order of predictors do not matter 
\end{itemize}

\end{frame}
%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Diagnostics}

\begin{frame}[fragile]{Why we need checks: summary(lm) isn't enough}

  <<>>=
    Ans <- read.csv(file = "Anscombe.csv")
  @
  
  <<anscombe, eval=TRUE, echo=FALSE>>=
  par(mfrow=c(2,2))
    lm1 <- lm(y ~ x , data=Ans[Ans$distri==1,])
    plot(Ans$x[Ans$distri==1], Ans$y[Ans$distri==1],
         xlim=range(Ans$x), ylim=range(Ans$y))
    abline(lm1, col="red")  
    
        lm2 <- lm(y ~ x , data=Ans[Ans$distri==2,])
    plot(Ans$x[Ans$distri==2], Ans$y[Ans$distri==2],
         xlim=range(Ans$x), ylim=range(Ans$y))
    abline(lm2, col="red")  
    
        lm3 <- lm(y ~ x , data=Ans[Ans$distri==3,])
    plot(Ans$x[Ans$distri==3], Ans$y[Ans$distri==3],
         xlim=range(Ans$x), ylim=range(Ans$y))
    abline(lm3, col="red")  
    
        lm4 <- lm(y ~ x , data=Ans[Ans$distri==4,])
    plot(Ans$x[Ans$distri==4], Ans$y[Ans$distri==4],
         xlim=range(Ans$x), ylim=range(Ans$y))
    abline(lm4, col="red")  
    
    
    par(mfrow=c(1,1))
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}{General approach}

\begin{center}
  \begin{tikzpicture}
    \node (sq) at (0,-1) {\color{red}{1. Scientific question}};
    \node (mo) at (0,-2) {2. Model and Statistical question};
    \draw[->, thick] (sq)--(mo);
    \node (dac) at (6,-2) {\color{red}{3. Data collection}};
    \draw[<->, thick] (mo)--(dac);

    \node (est) at (0,-3) {4. Estimation};
        \draw[->, thick] (mo)--(est);
    \node (unc) at (0,-3.5) {4.b Uncertainty and statistical significance};
    
    \node (che) at (0,-5) {\textbf{5. Diagnostic, check assumptions, prediction}};
        \draw[->, thick] (unc)--(che);
    \draw[->, thick] (che.west) to [out=150, in=210] (mo.west);

    \node (int) at (0,-6) {\color{red}{6. Interpret and think about the biology}};
        \draw[->, thick] (che)--(int);

  \draw[rounded corners, color=blue] (-4.5,-1.5) rectangle (4,-5.5);
  \node[anchor=north west] (r) at (-4.5,-1.5) {\includegraphics[width=0.1\textwidth]{Figures/r}};
  \end{tikzpicture}
  \end{center}
\end{frame}
%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear model basic assumptions}
Not necessarily wrong, but typical interpretation assumes:
 \begin{block}{}
     \begin{itemize}[<+->]
      \item Linear combination of parameters (including transformation, polynoms, interactions\dots)\\ \textit{Risk: biologically meaningless; e.g. static allometry}
      \item Predictor not perfectly correlated \\ \textit{Risk: Model won't run, unstable convergence, or huge SE}
       \item {\color{red!20!black}{Measurement error in predictors}}\\ \textit{Risk: bias estimates (underestimate with Gaussian error)}
       \item {\color{red!50!black}{Gaussian error distribution}}\\ \textit{Risk: Poor predictions}
       \item {\color{red!70!black}{Homoscedasticity (constant error variance)}}\\ \textit{Risk: Over-optimistic uncertainty, unreliable predictions}
       \item {\color{red!99!black}{Independence of error}}\\ \textit{Risk: Bias and over-optimistic uncertainty}
     \end{itemize}
 \end{block}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Why we need checks: missing a relationship}
  
  <<echo=FALSE, eval=FALSE>>=
    set.seed(123)
    pred <- runif(n = 100, 0, 10)
    obs <- 1 + 1*pred - 0.1*pred^2 + rnorm(100, sd = 0.5) 

    plot(x,obs)  
  
    summary(m0 <- lm(obs ~ x))
    plot(m0)
    
    write.csv(x = data.frame(obs=obs, predictor=pred), file = "forprediction.csv", row.names = FALSE)
  @

  <<>>=
   forprediction <- read.csv(file = "forprediction.csv")  
  @
  Does "predictor" predict "obs"? 
 \pause

  <<eval=FALSE>>=
    summary(lm(obs ~ 1 + predictor, data=forprediction) )
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Why we need checks: missing a relationship}
  Does "predictor" predict "obs"? Apparently not:
  <<>>=
    summary(lm(obs ~ 1 + predictor, data=forprediction) )
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Why we need checks: missing a relationship}

<<plot0, dev='tikz',echo=FALSE>>=
setPar()
plot(x=forprediction$predictor, y=forprediction$obs, xlab="predictor", ylab="obs")
@

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{How to check?}
  <<>>=
  m0 <- lm(obs ~ 1 + predictor, data=forprediction) 
  summary(m0)
  @
\end{frame}
%%%%%%%%%%%


\begin{frame}[fragile]{How to check?}
  <<>>=
  par(mfrow=c(2,2))
    plot(m0)
  par(mfrow=c(1,1))
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{How to check?}

<<>>=
m0 <- lm(obs ~ 1 + predictor, data=forprediction)
@
    \centering
<<plot0plusabline, dev='tikz', fig.width=10, out.height='0.6\\textheight'>>=
    setPar()
plot(x=forprediction$predictor, y=forprediction$obs, xlab="predictor", ylab="obs")
abline(m0, col="red", lwd=3) #simple prediction, without SE
@

\end{frame}
%%%%%%%%%%%

\begin{frame}{Check checklist}
  \begin{exampleblock}{}
    \begin{itemize}
      \item \textbf{Visualize your data}
      \item Residual in summary(): are they symmetrical?
      \item plot(lm):
        \begin{enumerate}
          \item trend residual/fitted?
          \item Normal residuals?
          \item trend in residual variance?
          \item outliers?
        \end{enumerate}
      \item Predictions: range and biological meaning
    \end{itemize}
  \end{exampleblock}
  
  
\end{frame}
%%%%%%%%%%

\begin{frame}[fragile]{Fix?}

Plot suggests a quadratic relationship

  <<eval=FALSE>>=
  lm(obs ~ 1 + predictor , data=forprediction)
  @
  \pause

  <<eval=FALSE>>=
  m1 <- lm(obs ~ 1 + predictor + I(predictor^2), data=forprediction)
  plot(m1)
  @

  How about prediction? (abline(m1) won't work here because not straight line)
\end{frame}
%%%%%%%%%%%%

\begin{frame}[fragile]{Introduction to prediction}

  <<>>=
  m1 <- lm(obs ~ 1 + predictor + I(predictor^2), data=forprediction)
  coef(m1)
  @
  
  \begin{alertblock}{Exercise}
    \begin{enumerate}
      \item Write mathematically the relationship between obs and predictor
      \item Input regression coefficients in there to predict "obs" from "predictor"
      \item Add a prediction line on the plot obs/predictor
    Is the fit satisfactory?
    \end{enumerate}
  \end{alertblock}
\end{frame}
%%%%%%%%%%%%

\begin{frame}[fragile]{Introduction to prediction}

  <<echo=FALSE>>=
  xx <- seq(from=0, to = 10, length.out = 100)
  predxx <- coef(m1)[1] + xx * coef(m1)[2] + (xx^2) * coef(m1)[3]
  @

  <<echo=FALSE>>=
  plot(x=forprediction$predictor, y=forprediction$obs, xlab="predictor", ylab="obs")
  lines(x = xx, y=predxx, col="red", lwd=3)
  @
\end{frame}
%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{Figures/brain.jpg}
  \end{center}
\end{frame}

\section{A puzzling but simple problem: Over-fit and collinearity}

\begin{frame}[fragile]{Over-fit and collinearity}
\begin{alertblock}{Small exercise}
Load Cdata.csv, fit models of y predited by x1 and x2, or x2 and x3.\\ Something is weird, what is going on? What to do?
\end{alertblock}

  <<echo=FALSE, eval=FALSE>>=
set.seed(123)
ns <- 35
x1 <- runif(n = ns, min = 0,max = 1)
x2 <- 1-x1 + rnorm(ns, 0, 0.001)
x3 <- x2/2

y <- x1 + rnorm(ns, sd = 0.7)

summary(lm(y ~ x1 + x2))
summary(lm(y ~ x2 + x3))

Cdata <- data.frame(y = y, x1=x1, x2=x2, x3 = x3)
write.csv(Cdata, file = "Cdata.csv", row.names = FALSE)
@

  <<echo=FALSE, eval=FALSE>>=
  cdata <- read.csv(file = "Cdata.csv")
  summary(lm(y ~ x1 + x2, data=cdata))
  summary(lm(y ~ x2 + x3, data=cdata))
@

\end{frame}
%%%%%%%%%%%

\section{Heteroschedasticity: the spooky word}

\begin{frame}[fragile]{Exponential data}

<<echo=FALSE>>=
set.seed(123)
x <- 2+rnorm(100)
y <- 1 + x + rnorm(100)
obs <- exp(y)

write.csv(data.frame(obs = obs, x=x), row.names = FALSE, file = "htrsdt.csv")
htrsdt <- read.csv("htrsdt.csv")
@

Load the dataset \texttt{htrsdt.csv}.

<<eval=FALSE>>=
plot(htrsdt$x, htrsdt$obs, ylim = c(-20, max(obs)))
abline(lm(obs ~ x, data=htrsdt))
summary(lm(obs ~ x, data=htrsdt))
plot(lm(obs ~ x, data=htrsdt))
@
\end{frame}
%%%%%%%%%%%


\begin{frame}[fragile]{Exponential data: prediction}

Make a prediction over the range of x, with prediction interval
\pause
"By-hand"
<<eval=FALSE>>=
lmhtrsdt <- lm(obs ~ x, data=htrsdt)
x <- seq(from=min(x), to=max(x), length.out = 100)
predhtrsdt <- coef(lmhtrsdt)[1] + x * coef(lmhtrsdt)[2]
@

\pause
Function predict
<<>>=
Xnewdata <- data.frame(x=seq(from=min(x), to=max(x),
                             length.out = 100))
Xpred <- predict(object =  lm(obs ~ x, data=htrsdt), newdata = Xnewdata,
                 se.fit = TRUE, interval = "prediction")
@
\pause
<<>>=
Xnewdata <- cbind(Xnewdata, Xpred)
@

<<eval=FALSE>>=
head(Xnewdata)
@

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Exponential data: prediction}

<<eval=TRUE, fig.width=10, out.height='0.6\\textheight'>>=
plot(x, obs, ylim = c(-100, max(obs)))
lines(Xnewdata$x, Xnewdata$fit.fit, col="red", lwd=3)
lines(Xnewdata$x, Xnewdata$fit.lwr, col="red", lty=2, lwd=3)
lines(Xnewdata$x, Xnewdata$fit.upr, col="red", lty=2, lwd=3)
@

\end{frame}
%%%%%%%%%%%
%
\begin{frame}[fragile]{Exponential data: confidence}

\textbf{Prediction interval:} Where the model predicts new data would be sampled, including variation unrelated to predictor\\
\pause
\textbf{Confidence interval:} Where your model predicts the MEAN new data would be sampled; or where is the true relationship with predictor
<<>>=
Xconf <- predict(object = lm(obs ~ x), newdata = Xnewdata,
                 se.fit = TRUE, interval = "confidence")
Xnewdata[,c("conf.lwr", "conf.upr")] <-  Xconf$fit[,2:3]
@


\end{frame}
%%%%%%%%%%%
\begin{frame}[fragile]{Exponential data: confidence}

<<eval=TRUE, fig.width=10, out.height='0.6\\textheight'>>=
plot(x, obs, ylim = c(-100, max(obs)))
lines(Xnewdata$x, Xnewdata$fit.fit, col="red", lwd=3)
lines(Xnewdata$x, Xnewdata$fit.lwr, col="red", lty=2, lwd=3)
lines(Xnewdata$x, Xnewdata$fit.upr, col="red", lty=2, lwd=3)
lines(Xnewdata$x, Xnewdata$conf.lwr, col="blue", lty=2, lwd=3)
lines(Xnewdata$x, Xnewdata$conf.upr, col="blue", lty=2, lwd=3)
@

\end{frame}
%%%%%%%%%%
%
\begin{frame}[fragile]{Exponential data: fix}
\begin{block}{Model sufficient to show positive relationship\dots BUT}
  \begin{itemize}
    \item no negative values should exist
    \item too many outliers
    \item too much uncertainty on the left\dots
  \end{itemize}
\end{block}

\begin{block}{Consequences:}
\begin{itemize}
\item impossible to understand the biological mechanism
\item impossible to predict future observations
\end{itemize}
\end{block}

What to do?
\end{frame}
%%%%%%%%%%M

\begin{frame}[fragile]{Exponential data: fix}

Log-transform
<<eval=FALSE>>=
plot(x, log(obs)); abline(lm(log(obs) ~ x))
summary(lm(log(obs) ~ x))
plot(lm(log(obs) ~ x))
@

\pause
By the way, the simulation process:
<<echo=TRUE, eval=FALSE>>=
set.seed(123)
x <- 2+rnorm(100)  
y <- 1 + x + rnorm(100)
obs <- exp(y)
@

\end{frame}
%%%%%%%%%%M

\begin{frame}[fragile]{Exponential data: other fix}
Non-parametric statistics:\\ 
Rank observations, do greater ranks go together?\\ \pause
Case of two continuous variable: Spearman's rank correlation
\pause
<<>>=
cor.test(x = x, y = obs, method = "spearman")
@
\pause
Significant positive correlation confirmed, BUT, no biological mechanism, little predictive power.

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Practice lm() with parasites}

<<echo=FALSE, eval=FALSE>>=
  set.seed(123)
  ns <- 135
  x2 <- sample(x = 1:3, size = ns, replace = TRUE)
  x1 <- rnorm(ns, 0, 1) + c(-1,0,1)[x2]
  x3 <- rnorm(ns, 5, 0.1)#no effect
  x4 <- sample(x=0:1, size=ns, replace=TRUE)#unobserved
  y <- 2 + 0.3*x1 + 0.5*(x2/2)+ x4 + rnorm(ns, sd = 0.7)
  lamb <- exp(y)
  obs <- abs(sapply(X = lamb, FUN = function(x) {rpois(n = 1, lambda = x)}) + rnorm(ns, sd=0.5))

para <- data.frame(Parasite_Mass = obs, Individual_Size = x1 +6, Location = c("A","B","C")[x2], Fur_Darkness = x3)

plot(para$Parasite_Mass, x=para$Individual_Size)
summary(glm(Parasite_Mass ~ Individual_Size + as.factor(Location) + Fur_Darkness, data=para, family="quasipoisson"))

summary(lm(Parasite_Mass ~ Individual_Size, data=para))
summary(lm(Parasite_Mass ~ Individual_Size+as.factor(Location), data=para))
hist(resid(lm(Parasite_Mass ~ Individual_Size + as.factor(Location) + Fur_Darkness, data=para)))
write.csv(x = para, file = "Para.csv", row.names = FALSE)
@
  
  \begin{alertblock}{What explains variation in parasitic load?}
  You collected ecto-parasites on some furry large mammals at three locations. Parasites break easily when we collect them and are impossible to count, so we decide to measure parasitic load as their mass. \textbf{Why do some mammals have larger parasitic load?} \pause
    \begin{itemize}
      \item Load the \texttt{Para.csv} data (don't forget: str(), summary(), plot()\dots)
      \item Model \verb+Parasite_Mass+ using \texttt{lm()}
      \item Find what variables predict \verb+Parasite_Mass+
      \item How good are your models? Assumptions? Prediction?
      \item What biological interpretation can you imagine?
      \end{itemize}
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Linear model basic assumptions}
Not necessarily wrong, but typical interpretation assumes:
 \begin{block}{}
     \begin{itemize}
      \item Linear combination of parameters (including transformation, polynoms, interactions\dots)
      \item Predictor not perfectly correlated 
       \item {\color{red!20!black}{Measurement error in predictors}}
       \item {\color{red!50!black}{Gaussian error distribution}}
       \item {\color{red!70!black}{Homoscedasticity (constant error variance)}}
       \item {\color{red!99!black}{Independence of error}}
     \end{itemize}
 \end{block}
 
 \pause
 \begin{exampleblock}{TEASER: The most difficult problems are the reason for}
  \begin{itemize}[<+->]
    \item Multiple regression
    \item Generalized Linear Models
    \item (Generalized) Linear Mixed Models
    \item Non-linear models
  \end{itemize}
 \end{exampleblock}
\end{frame}
%%%%%%%%%%%

\section{If time permits: multiple regression}

\begin{frame}{Improvisation}

\end{frame}
%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \section*{Feedback wanted!}
% 
% \begin{frame}{What do you want to learn about?}
% 
% \begin{alertblock}{Topics}
%   \begin{itemize}
%     \item 
%   \end{itemize}
% \end{alertblock}
% 
% \end{frame}
% %%%%%%%%%%%


\end{document}
